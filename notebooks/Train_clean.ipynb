{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # GPU index\n",
    "\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\"..\", \"data\"))\n",
    "sys.path.insert(1, os.path.join(\"..\", \"utils\"))\n",
    "from data_utils import Dataset\n",
    "from plot_utils import plot_image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder (update the variable to your path).\n",
    "path_data=os.path.join(\"..\", \"data\")\n",
    "# Seed value\n",
    "seed=704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing class: Cloud: 146it [00:08, 16.32it/s]\n",
      "Parsing class: Edge: 97it [00:04, 22.60it/s]\n",
      "Parsing class: Good: 61it [00:03, 19.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cloud</th>\n",
       "      <td>98</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge</th>\n",
       "      <td>73</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>41</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train  valid  test\n",
       "cloud     98     21    27\n",
       "edge      73     13    11\n",
       "good      41     12     8"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=Dataset(path_data=path_data, seed=seed)\n",
    "dataset.read_data()\n",
    "dataset.get_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "# Train loader\n",
    "train_loader = DataLoader(dataset.get_split(\"train\"), batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Cross validation data loader\n",
    "valid_loader = DataLoader(dataset.get_split(\"valid\"), batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Test data loader\n",
    "test_loader = DataLoader(dataset.get_split(\"test\"), batch_size=batch_size, pin_memory=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = ('cloud', 'edge', 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# def resize_tensor_images(images, size=(256, 128)):\n",
    "#     # Resize the batch of images\n",
    "#     return F.interpolate(images, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "def compute_mean_std(loader):\n",
    "    # Computation of mean and standard deviation of batches\n",
    "\n",
    "\n",
    "    num_of_pixels_per_channel = 212 * 256 * 128  # This remains the same since we're doing it per channel\n",
    "\n",
    "    total_sum_channels = torch.tensor([0.0, 0.0, 0.0])  # One for each channel\n",
    "    for batch in loader:\n",
    "        # Assuming batch[0] contains image data in the shape [batch, channels, height, width]\n",
    "        total_sum_channels += batch[0].view(batch[0].size(0), batch[0].size(1), -1).sum(dim=[0, 2])\n",
    "\n",
    "    mean = total_sum_channels / num_of_pixels_per_channel\n",
    "\n",
    "    sum_of_squared_error_channels = torch.tensor([0.0, 0.0, 0.0])\n",
    "    for batch in loader:\n",
    "        # We subtract the mean for each channel and then square it\n",
    "        sum_of_squared_error_channels += ((batch[0] - mean.view(1, -1, 1, 1)) ** 2).view(batch[0].size(0), batch[0].size(1), -1).sum(dim=[0, 2])\n",
    "\n",
    "    std = torch.sqrt(sum_of_squared_error_channels / num_of_pixels_per_channel)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "def normalize_images(images, mean, std):\n",
    "    # Normalizing images with previously computed mean and standard deviation\n",
    "    normalized_images = (images - mean.view(-1, 1, 1)) / std.view(-1, 1, 1)\n",
    "    return normalized_images\n",
    "    \n",
    "def tensor_to_numpy(tensor):\n",
    "    # Rescale the tensor to 0-1 range\n",
    "    tensor = tensor - tensor.min()\n",
    "    tensor = tensor / tensor.max()\n",
    "    # Move the tensor to CPU if it's on GPU\n",
    "    tensor = tensor.cpu()\n",
    "    # Convert to numpy and transpose from CxHxW to HxWxC for visualization\n",
    "    numpy_image = tensor.numpy()\n",
    "    numpy_image = np.transpose(numpy_image, (1, 2, 0))\n",
    "\n",
    "    return numpy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = compute_mean_std(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data_loader, mean, std):\n",
    "    UNPRO_batches = []\n",
    "    batches = []\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        images, labels = batch\n",
    "        # resized_images = resize_tensor_images(images)\n",
    "        UNPRO_batches.append((images, labels))\n",
    "        normalized_alldata_images = normalize_images(images, mean, std)\n",
    "\n",
    "        # Append the normalized images and their corresponding labels to the list\n",
    "        batches.append((normalized_alldata_images, labels))\n",
    "    return UNPRO_batches, batches\n",
    "\n",
    "UNPRO_batches_TRL, batches_TRL = normalization(train_loader, mean, std)\n",
    "UNPRO_batches_VAL, batches_VAL = normalization(valid_loader, mean, std)\n",
    "UNPRO_batches_TST, batches_TST = normalization(test_loader, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(seed=42)\n",
    "torch.cuda.manual_seed(seed=42)\n",
    "\n",
    "# Definition of the neural network class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Definition of the first convolutional layer with batch normalization, rectified linear unit function and maxpooling.\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # Definition of the second convolutional layer with batch normalization, rectified linear unit function, maxpooling and average pooling.\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(3, 3),\n",
    "        )\n",
    "        # Definition of the fully connected classifier layer with rectified linear unit function.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=5120,\n",
    "                      out_features=164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=164,\n",
    "                      out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32,\n",
    "                      out_features=3),\n",
    "        )\n",
    "\n",
    "    # Changing forward function of the nn.Module class to the defined layers.\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "# Making an instance of the neural network and setting it to the gpu    \n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Calculating the count of cloud, edge and good images in the training dataset\n",
    "cloud_count = 0\n",
    "edge_count = 0\n",
    "good_count = 0\n",
    "for image, label in dataset.get_split(\"train\"):\n",
    "    if label == 0: \n",
    "        cloud_count += 1\n",
    "    if label == 1: \n",
    "        edge_count += 1\n",
    "    if label == 2: \n",
    "        good_count += 1\n",
    "\n",
    "# Setting the loss with weights calculated by the cound of cloud, edge and good images and setting it to the gpu.\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1, cloud_count/edge_count, cloud_count/good_count]).to(device))\n",
    "# Setting the optimizer (Stochastic gradient decent) with a learning rate and a momentum\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00003, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "# Defining the accuracy function used to calculate accuracies in each step\n",
    "accuracy_fn = Accuracy(task=\"multiclass\", num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train step function to be used in the training loop\n",
    "def train_step(model: torch.nn.Module,\n",
    "               batches,\n",
    "               loss_fn,\n",
    "               optimizer,\n",
    "               accuracy,\n",
    "               device: torch.device = device):\n",
    "    # Setting the model to training mode\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Looping over the multiple batches of the training dataset \n",
    "    for batch, (images, labels) in enumerate(batches, 0):\n",
    "        # Setting the image data to the gpu\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Putting the images through the model\n",
    "        y_logits = model(images)\n",
    "\n",
    "        # Calculate loss on 1 batch of data\n",
    "        loss = loss_fn(y_logits, labels)\n",
    "        # Calculating the accuracy on 1 batch of data\n",
    "        acc = accuracy(y_logits.argmax(dim=-1), labels)\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "\n",
    "        # Setting optimizer gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        # Calculating gradients of the weights in the neural network on the loss\n",
    "        loss.backward()\n",
    "        # Stepping in the negative gradient direction to optimize the weights\n",
    "        optimizer.step()\n",
    "    # Calculating accuracy of the whole dataset\n",
    "    train_acc = train_acc / len(batches)\n",
    "    # Calculating loss of the whole dataset\n",
    "    train_loss = train_loss / len(batches)\n",
    "\n",
    "    # Printing loss and accuracy\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Training Accuracy: {train_acc*100:.4f}\")\n",
    "    return train_acc, train_loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model,\n",
    "              batches,\n",
    "              loss_fn,\n",
    "              accuracy,\n",
    "              device: torch.device = device):\n",
    "    validation_loss, validation_acc = 0, 0\n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Setting the model to inference mode (gradients aren't computed)\n",
    "    with torch.inference_mode():\n",
    "        # Looping over batches\n",
    "        for images_validation, labels_validation in batches:\n",
    "            # Setting image data to the gpu\n",
    "            images_validation, labels_validation = images_validation.to(device), labels_validation.to(device)\n",
    "            # Running the images through the model\n",
    "            validation_logits = model(images_validation)\n",
    "            # Calculating loss and accuracy for one batch\n",
    "            validation_loss += loss_fn(validation_logits, labels_validation)\n",
    "            validation_acc += accuracy(validation_logits.argmax(dim=-1), labels_validation)\n",
    "        # Calculating loss and accuracy for all batches\n",
    "        validation_loss /= len(batches)\n",
    "        validation_acc /= len(batches)\n",
    "    # Printing loss and accuracy\n",
    "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_acc*100:.4f}%\")\n",
    "    return validation_acc, validation_loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model,\n",
    "              batches,\n",
    "              loss_fn,\n",
    "              accuracy,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Setting the model to inference mode (gradients aren't computed)\n",
    "    with torch.inference_mode():\n",
    "        # Looping over batches\n",
    "        for images_test, labels_test in batches:\n",
    "            # Setting image data to the gpu\n",
    "            images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "            # Running the images through the model\n",
    "            test_logits = model(images_test)\n",
    "            # Calculating loss and accuracy for one batch\n",
    "            test_loss += loss_fn(test_logits, labels_test)\n",
    "            test_acc += accuracy(test_logits.argmax(dim=-1), labels_test)\n",
    "        # Calculating loss and accuracy for all batches\n",
    "        test_loss /= len(batches)\n",
    "        test_acc /= len(batches)\n",
    "    # Printing loss and accuracy\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.4f}%\")\n",
    "    return test_acc, test_loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# Number of loops the model will train for\n",
    "epochs = 650\n",
    "# Creating lists for loss and accuracy data\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "net.to(device)\n",
    "# Performing training\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_accuracy, train_loss = train_step(model=net,\n",
    "               batches=batches_TRL,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               accuracy=accuracy_fn,\n",
    "               device=device\n",
    "               )\n",
    "    val_accuracy, val_loss = validation_step(model=net,\n",
    "              batches=batches_VAL,\n",
    "              loss_fn=loss_fn,\n",
    "              accuracy=accuracy_fn,\n",
    "              device=device)\n",
    "    test_accuracy, test_loss = test_step(model=net,\n",
    "              batches=batches_TST,\n",
    "              loss_fn=loss_fn,\n",
    "              accuracy=accuracy_fn,\n",
    "              device=device)\n",
    "    # Adding losses and accuracies to lists\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to a .pth file.\n",
    "\n",
    "The usage for this is that we at least explain how many seeds were used, what the batch size was, as well as the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "txt = './e{}_b{}_s{}.pth'.format(epochs, batch_size, seed)\n",
    "PATH = txt\n",
    "torch.save(net.state_dict(), PATH)\n",
    "# Loading the model (not necessary but performed as test)\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for concatenating batches\n",
    "\n",
    "These make sure that we are not just using a batch for confusion matrices and plotting, but the whole split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_batches(batches):\n",
    "    images_norm = torch.tensor([])\n",
    "    # For loop over the batches\n",
    "    for i, batch in enumerate(batches):\n",
    "        images_batch, labels_batch = batch\n",
    "        if i == 0:\n",
    "            images_norm = images_batch\n",
    "            labels_norm = labels_batch\n",
    "        else:\n",
    "            images_norm = torch.cat((images_norm, images_batch))\n",
    "            labels_norm = torch.cat((labels_norm, labels_batch))\n",
    "    return images_norm, labels_norm\n",
    "\n",
    "# Concatenate all the batches of the different types of splits.\n",
    "images_norm_TRL, labels_norm_TRL = concatenate_batches(batches_TRL)\n",
    "images_norm_VAL, labels_norm_VAL = concatenate_batches(batches_VAL)\n",
    "images_norm_TST, labels_norm_TST = concatenate_batches(batches_TST)\n",
    "\n",
    "images_og_TST, labels_og_TST = concatenate_batches(UNPRO_batches_TST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to the cpu\n",
    "net.to('cpu')\n",
    "# Running test data through model\n",
    "outputs_TST = net(images_norm_TST.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted classes of the model\n",
    "_, predicted_TST = torch.max(outputs_TST, 1)\n",
    "\n",
    "predictions_TST = [classes[predicted_TST[j]] for j in range(len(predicted_TST))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix \n",
    "The confusion matrix of the test split is plotted using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import MulticlassConfusionMatrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "class_mapping = {'cloud': 0, 'edge':1, 'good':2}\n",
    "\n",
    "# Set x and y axis to corresponding predictions and true labels\n",
    "input = predictions_TST\n",
    "input = torch.tensor([class_mapping[class_name] for class_name in input])\n",
    "target = torch.tensor(labels_og_TST)\n",
    "\n",
    "# make the confusion matrix\n",
    "metric = MulticlassConfusionMatrix(3)\n",
    "metric.update(input=input, target=target)\n",
    "confmat = metric.compute()\n",
    "total_sum = np.sum(confmat.numpy())\n",
    "\n",
    "# Group bad parts of the confusion matrix together for later use in true/false positives/negatives.\n",
    "TN_confmat = np.array([[np.sum(confmat.numpy()[:2, :2]) / total_sum, np.sum(confmat.numpy()[:2, 2]) / total_sum], \n",
    "                               [np.sum(confmat.numpy()[2, :2]) / total_sum, np.sum(confmat.numpy()[2, 2]) / total_sum]])\n",
    "\n",
    "# Convert the confusion matrix to a form for presentation with seaborn.\n",
    "confmat = pd.DataFrame(confmat)\n",
    "confmat.rename(index={0: 'True Cloud', 1: 'True Edge', 2: 'True Good'}, inplace=True)\n",
    "confmat.rename(columns={0: 'pred Cloud', 1: 'pred Edge', 2: 'pred Good'}, inplace=True)\n",
    "\n",
    "\n",
    "sn.heatmap(confmat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting each image, its normalized version, the true label, and the prediction from the model next to eachother for all images.\n",
    "\n",
    "In this case, blue predictions correspond to correct predictions while yellow correspond to wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(46, 4, figsize=(12, 4 * 46))\n",
    "\n",
    "# For loop to showcase all four pieces of information for all images.\n",
    "for i in range(len(images_norm_TST)):\n",
    "    og_img = tensor_to_numpy(images_og_TST[i])\n",
    "    norm_img = tensor_to_numpy(images_norm_TST[i])\n",
    "    \n",
    "    axs[i, 0].imshow(og_img)\n",
    "    axs[i, 0].axis('off')\n",
    "\n",
    "    # Plot the second image\n",
    "    axs[i, 1].imshow(norm_img)  # Assuming there are always pairs of images\n",
    "    axs[i, 1].axis('off')\n",
    "\n",
    "    # Plot the first text box\n",
    "    axs[i, 2].text(0.5, 0.5, classes[labels_norm_TST[i]], ha='center', va='center', fontsize=12, color='black')\n",
    "    axs[i, 2].axis('off')\n",
    "\n",
    "    # Plot the second text box\n",
    "    axs[i, 3].text(0.5, 0.5, 'prediction: ' + str(predictions_TST[i]), ha='center', va='center', fontsize=12, color='black')\n",
    "    axs[i, 3].axis('off')\n",
    "    if predictions_TST[i] == classes[labels_norm_TST[i]]:\n",
    "        row_color = 'lightblue'  # Set to blue if the condition is true\n",
    "    else:\n",
    "        row_color = 'yellow'  # Set to yellow if the condition is false\n",
    "\n",
    "    rect = patches.Rectangle((0, 0), 1, 1, linewidth=0, edgecolor='none', facecolor=row_color, alpha=0.5)\n",
    "    axs[i, 0].add_patch(rect)\n",
    "    axs[i, 1].add_patch(patches.Rectangle((0, 0), 1, 1, linewidth=0, edgecolor='none', facecolor=row_color, alpha=0.5))\n",
    "    axs[i, 2].add_patch(patches.Rectangle((0, 0), 1, 1, linewidth=0, edgecolor='none', facecolor=row_color, alpha=0.5))\n",
    "    axs[i, 3].add_patch(patches.Rectangle((0, 0), 1, 1, linewidth=0, edgecolor='none', facecolor=row_color, alpha=0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and cross-validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'ro-', label='Cross-validation loss')\n",
    "plt.title('Training and Cross-validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing false negatives in the code\n",
    "Here the percentage of false negatives is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = 0\n",
    "total_labels_TST = 0\n",
    "\n",
    "# For loop over all predictions true test cases.\n",
    "for i in range(len(predictions_TST)):\n",
    "    if predictions_TST[i] != 'good' and labels_og_TST[i] == 2:\n",
    "        false_negatives += 1\n",
    "    if labels_og_TST[i] == 2:\n",
    "        total_labels_TST += 1\n",
    "\n",
    "percentage_false_negatives = 100 * false_negatives / total_labels_TST\n",
    "print(\"The percentage of false negatives is : \", percentage_false_negatives, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results to an excel file\n",
    "We put the seed, accuracies, losses, and true/false positives/negatives into an excel such that averaging can be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define what you want to put into the excel.\n",
    "data_current_run = {'Seed' : seed, 'Test accuracy': test_accuracies[-1].item(), 'Validation accuracy': val_accuracies[-1].item(), 'Train Accuracy': train_accuracies[-1].item(), 'Validation loss': val_losses[-1].item(),  'Train loss': train_losses[-1].item(), 'True Negative': TN_confmat[0, 0], 'False Negative': TN_confmat[1, 0], 'False Positive': TN_confmat[0, 1], \"True Positive\" : TN_confmat[1, 1]}\n",
    "\n",
    "# Read the Excel file\n",
    "excel_file = 'seed_averaging.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Write to first available row\n",
    "first_available_row = len(df)\n",
    "\n",
    "# Append your data to the DataFrame\n",
    "df = pd.concat([df, pd.DataFrame([data_current_run])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame back to Excel\n",
    "df.to_excel(excel_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OMICRON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
